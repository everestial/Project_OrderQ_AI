{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Processing with OrderQ AI\n",
    "\n",
    "This notebook demonstrates how to use the OrderQ AI model to process restaurant orders from natural language text into structured JSON.\n",
    "We will also show how to read input orders from a text file and output the processed results as a TSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from typing import Dict, Optional\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def initialize_model(model_path: str = './trained_model'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def validate_and_fix_json(text: str) -> Optional[Dict]:\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    fixed_text = text.strip()\n",
    "    if not fixed_text.startswith('{'):\n",
    "        fixed_text = '{' + fixed_text\n",
    "    if not fixed_text.endswith('}'):\n",
    "        fixed_text = fixed_text + '}'\n",
    "    try:\n",
    "        fixed_text = re.sub(r'(\\w+):', r'\"\\1\":', fixed_text)\n",
    "        fixed_text = fixed_text.replace(\"'\", '\"')\n",
    "        result = json.loads(fixed_text)\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def process_order(model, tokenizer, device, order_text: str) -> Dict[str, Optional[str]]:\n",
    "    input_text = 'extract order: ' + order_text\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True, padding=True).to(device)\n",
    "    outputs = model.generate(**inputs, max_length=256, num_beams=8, early_stopping=True)\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    logger.info(f'Raw output: {raw_output}')\n",
    "    parsed_json = validate_and_fix_json(raw_output)\n",
    "    if parsed_json:\n",
    "        return {'status': 'success', 'result': json.dumps(parsed_json)}\n",
    "    else:\n",
    "        return {'status': 'error', 'raw_output': raw_output}\n",
    "\n",
    "def process_orders_from_file(input_file: str, output_file: str, model, tokenizer, device):\n",
    "    logger.info(f'Reading orders from {input_file}')\n",
    "    with open(input_file, 'r') as f:\n",
    "        order_texts = [line.strip() for line in f if line.strip()]\n",
    "    results = []\n",
    "    for order_text in order_texts:\n",
    "        result = process_order(model, tokenizer, device, order_text)\n",
    "        results.append(result)\n",
    "    df = pd.DataFrame(results)\n",
    "    logger.info(f'Saving results to {output_file}')\n",
    "    df.to_csv(output_file, sep='\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './trained_model'\n",
    "model, tokenizer, device = initialize_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Orders from File\n",
    "\n",
    "We'll read orders from `orders.txt` and save the structured output to `processed_orders.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 03:14:15,354 - INFO - Reading orders from data/orders.txt\n",
      "2025-07-18 03:14:21,200 - INFO - Raw output: \"customer_name\": \"John Smith\", \"order_type\": \"delivery\", \"total_number_of_different_items\": 2, \"order_items_name\": \"pizza|diet cokes\", \"order_items_quantity\": \"2|3\", \"order_items_modifications\": \"large|extra cheese|\", \"order_notes\": null\n",
      "2025-07-18 03:14:27,328 - INFO - Raw output: \"customer_name\": \"Sarah Johnson\", \"order_type\": \"takeout\", \"total_number_of_different_items\": 2, \"order_items_name\": \"burger|coffees\": \"1|2\", \"order_items_quantity\": \"1|2\", \"order_items_modifications\": \"|no onions|\", \"order_notes\": null\n",
      "2025-07-18 03:14:33,198 - INFO - Raw output: \"customer_name\": \"Mike Davis\", \"order_type\": \"delivery\", \"total_number_of_different_items\": 2, \"order_items_name\": \"chicken sandwiches|extra pickles\", \"order_items_quantity\": \"3|4\", \"order_items_modifications\": \"|extra pickles\", \"order_notes\": null\n",
      "2025-07-18 03:14:39,807 - INFO - Raw output: \"customer_name\": \"Anna Wilson\", \"order_type\": \"delivery\", \"total_number_of_different_items\": 2, \"order_items_name\": \"salad|iced teas\", \"order_items_quantity\": \"1|2\", \"order_items_modifications\": \"|no tomatoes|\", \"order_notes\": null\n",
      "2025-07-18 03:14:46,370 - INFO - Raw output: \"customer_name\": \"Tom Brown\", \"order_type\": \"delivery\", \"total_number_of_different_items\": 2, \"order_items_name\": \"pizza|sprite\", \"order_items_quantity\": \"1|2\", \"order_items_modifications\": \"large meat lovers|\"; \"order_notes\": null\n",
      "2025-07-18 03:14:52,967 - INFO - Raw output: \"customer_name\": \"Lisa Garcia\", \"order_type\": \"takeout\", \"total_number_of_different_items\": 3, \"order_items_name\": \"fish tacos|fries|limeade\", \"order_items_quantity\": \"2|1|1\", \"order_items_modifications\": \"|hot sauce||\", \"order_notes\": null\n",
      "2025-07-18 03:14:59,252 - INFO - Raw output: \"customer_name\": \"David Lee\", \"order_type\": \"delivery\", \"total_number_of_different_items\": 3, \"order_items_name\": \"chicken sandwich|garden salad|water\", \"order_items_quantity\": \"1|1|1\", \"order_items_modifications\": \"grilled||\", \"order_notes\": null\n",
      "2025-07-18 03:15:05,995 - INFO - Raw output: \"customer_name\": \"Jennifer Martinez\", \"order_type\": \"takeout\", \"total_number_of_different_items\": 2, \"order_items_name\": \"taco|soda\", \"order_items_quantity\": \"4|2\", \"order_items_modifications\": \"|mild sauce|\", \"order_notes\": null\n",
      "2025-07-18 03:15:12,569 - INFO - Raw output: \"customer_name\": \"Robert Wilson\", \"order_type\": \"delivery\", \"total_number_of_different_items\": 3, \"order_items_name\": \"burger|onion rings|milkshake\", \"order_items_quantity\": \"1|1|1\", \"order_items_modifications\": \"|extra bacon||\", \"order_notes\": null\n",
      "2025-07-18 03:15:18,707 - INFO - Raw output: \"customer_name\": \"Maria Rodriguez\", \"order_type\": \"delivery\", \"total_number_of_different_items\": 2, \"order_items_name\": \"pizza|sprite\", \"order_items_quantity\": \"2|1\", \"order_items_modifications\": \"vegetarian|large\", \"order_notes\": null\n",
      "2025-07-18 03:15:18,708 - INFO - Saving results to data/processed_orders_02.tsv\n"
     ]
    }
   ],
   "source": [
    "input_file = 'data/orders.txt'\n",
    "# output_file = 'processed_orders.tsv'\n",
    "output_file = 'data/processed_orders_02.tsv'\n",
    "\n",
    "process_orders_from_file(input_file, output_file, model, tokenizer, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
